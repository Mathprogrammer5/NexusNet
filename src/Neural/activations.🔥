# defines activation functions and a few aliases
from .datastruct import Vector

alias E: Float64 = 2.71828174591064453125

alias activation_fn = fn(borrowed x: Float64, borrowed a: Float64) -> Float64
alias vec_activation_fn = fn(borrowed x: Vector, borrowed a: Float64) -> Vector

# defines function for sum used in softmax
@always_inline
fn smsum(vec: Vector, temp: Float64) -> Float64:
  var val: Float64 = 0
  for j in range(len(vec)):
    val += E ** (temp * vec[j])
  return val

# defines relu activation function
@always_inline
fn float_relu(borrowed x: Float64, borrowed a: Float64 = 1) -> Float64:
  let val = x if x > 0 else 0
  return val

alias relu = Vector.vector_applicable(float_relu)

alias RELU: Int16 = 0

# defines leakyrelu activation function
@always_inline
fn float_leakyrelu(borrowed x: Float64, borrowed a: Float64 = 1) -> Float64:
  let val: Float64 = x if x > 0 else a*x
  return val

alias leakyrelu = Vector.vector_applicable(float_leakyrelu)

alias LRELU: Int16 = 1

# defines elu activation function
@always_inline
fn float_elu(borrowed x: Float64, borrowed a: Float64 = 1) -> Float64:
  let val = x if x > 0 else a*(E**x-1)
  return val

alias elu = Vector.vector_applicable(float_elu)

alias ELU: Int16 = 2

# defines sigmoid activation function
@always_inline
fn float_sigmoid(borrowed x: Float64,borrowed a: Float64 = 1) -> Float64:
  return 1/(1+E**(-x))

alias sigmoid = Vector.vector_applicable(float_sigmoid)

alias SIG: Int16 = 3

# defines softmax activation function
@always_inline
fn softmax(borrowed x: Vector, borrowed a: Float64 = 1) raises escaping -> Vector:
  var val = Vector()
  let sum = smsum(x, a)
  for i in range(len(x)):
    val.append(
    E ** (a * x[i]) / sum
    )

  return val

alias SMAX: Int16 = 4

# defines tanh activation function
@always_inline
fn float_tanh(borrowed x: Float64, borrowed a: Float64 = 1) -> Float64:
  return (E**x-E**(-x))/(E**x+E**(-x))

alias tanh = Vector.vector_applicable(float_tanh)

alias TANH: Int16 = 5
