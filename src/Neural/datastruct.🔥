# replaces the datastruct library for the time being (https://github.com/Mathprogrammer5/Mojo-Datastruct), 
# due to a mojo bug that does not allow one to import local packages into a custom package
from tensor import TensorShape
from random import seed, rand

alias bool = DType.bool
'''
This is an alias for your(and my) convienience.

It represents a boolean data type.
'''

alias si8 = DType.int8
'''
This is an alias for your(and my) convienience.

It represents a signed integer data type whose bitwidth is 8.
'''

alias si16 = DType.int16
'''
This is an alias for your(and my) convienience.

It represents a signed integer data type whose bitwidth is 16.
'''

alias si32 = DType.int32
'''
This is an alias for your(and my) convienience.

It represents a signed integer data type whose bitwidth is 32.
'''

alias si64 = DType.int64
'''
This is an alias for your(and my) convienience.

It represents a signed integer data type whose bitwidth is 64.
'''

alias ui8 = DType.uint8
'''
This is an alias for your(and my) convienience.

It represents an unsigned integer data type whose bitwidth is 8.
'''

alias ui16 = DType.uint16
'''
This is an alias for your(and my) convienience.

It represents an unsigned integer data type whose bitwidth is 16.
'''

alias ui32 = DType.uint32
'''
This is an alias for your(and my) convienience.

It represents an unsigned integer data type whose bitwidth is 32.
'''

alias ui64 = DType.uint64
'''
This is an alias for your(and my) convienience.

It represents an unsigned integer data type whose bitwidth is 64.
'''

alias f16 = DType.float16
'''
This is an alias for your(and my) convienience.

It represents a float data type whose bitwidth is 16.
'''

alias f32 = DType.float32
'''
This is an alias for your(and my) convienience.

It represents a float data type whose bitwidth is 32.
'''

alias f64 = DType.float64
'''
This is an alias for your(and my) convienience.

It represents a float data type whose bitwidth is 64.
'''

alias tf32 = DType.tensor_float32
'''
This is an alias for your(and my) convienience.

It represents a special float format supported by NVIDIA Tensor Cores.

This type is only available on NVIDIA GPUs.
'''

trait TensorWrapper:
  '''
  `Tensorwrapper` summarizes all types, 
  that 'wrap' around a `data` `Tensor`.
  '''

  fn __init__(inout self) raises:
    ...

  fn __copyinit__(inout self, borrowed other: Self):
    ...
	
  fn __moveinit__(inout self, owned other: Self):
    ...

trait MDTensorWrapper(TensorWrapper):
  
  '''
  `TensorWrappers` conforming to this trait have more than one dimension 
  and can be flattened out to a `Vector`.
  '''
  pass

struct Vector[type: DType](TensorWrapper, Sized):
  '''Defines a wrapper to handle mathematical vectors.'''
  
  var data: Tensor[type]
  var size: Int
  var _offset: Int

  fn __init__(inout self) raises:
    self.size = 0
    self._offset = 0
    self.data = Tensor[type]()

  fn __init__(inout self, owned size: Int) raises:
    self.size = size
    self._offset = 0
    self.data = Tensor[type](size)

  fn __init__(inout self, data: Tensor[type]) raises:
    if data.rank() != 1:
      raise Error("Unable to convert given data to a vector due to rank mismatch")
    self.data = data
    self._offset = 0
    self.size = data.dim(0)

  fn __del__(owned self):
    self.data.__del__()
    self.size.__del__()
    self._offset.__del__()

  fn __copyinit__(inout self, borrowed other: Self):
    self.data.__copyinit__(other.data)
    self._offset = other._offset
    self.size = other.size

  fn __moveinit__(inout self, owned other: Self):
    self.data.__moveinit__(other.data)
    self._offset = other._offset
    self.size = other.size

  fn __getitem__(self, index: Int) -> SIMD[type, 1]:
    return self.data[index]

  fn __len__(self) -> Int:
    return self.size

  fn __iter__(self) raises -> Self:
    return self.data

  fn __next__(inout self) raises -> SIMD[type,1]:
    if self._offset >= self.size:
      raise Error("Index out of range of vector")
    self._offset += 1
    return self.data[self._offset]

  @always_inline
  fn __ipow__(inout self, exponent: Int) raises:
    self.data.__ipow__(exponent)

  @always_inline
  fn __pow__(self, exponent: Int) raises -> Self:
    return Self(self.data ** exponent)

  @always_inline
  fn __truediv__(self, scalar: SIMD[type,1]) raises -> Self:
    return Self(self.data / scalar)

  @always_inline
  fn __rtruediv__(self, scalar: SIMD[type,1]) raises -> Self:
    return Self(self.data / scalar)

  @always_inline
  fn __truediv__(self, vec: Self) raises -> Self:
    if not self.dim_match(vec):
      raise Error("Unable to divide by vector due to dimension mismatch")

    return Self(self.data / vec.data)

  @always_inline
  fn __rtruediv__(self, vec: Self) raises -> Self:
    if not self.dim_match(vec):
      raise Error("Unable to divide by vector due to dimension mismatch")
    
    return Self(self.data / vec.data)

  @always_inline
  fn __mul__(self, vec: Self) raises -> SIMD[type,1]:
    if not self.dim_match(vec):
      raise Error("Unable to multiply vectors due to dimension mismatch")

    return (self.data * vec.data)[0]

  @always_inline
  fn __rmul__(self, vec: Self) raises -> SIMD[type,1]:
    return self.__mul__(vec)

  @always_inline
  fn __sub__(self, vec: Self) raises -> Self:
    if not self.dim_match(vec):
      raise Error("Unable to subtract vector due to dimension mismatch")
    
    return Self(self.data - vec.data)

  @always_inline
  fn __rsub__(self, vec: Self):
    pass

  @always_inline
  fn __add__(self, vec: Self) raises -> Self:
    if not self.dim_match(vec):
      raise Error("Unable to add vectors due to dimension mismatch")
    
    return Self(self.data + vec.data)

  @always_inline
  fn __eq__(self, vec: Self) -> Bool:
    if self.data != vec.data: return False
    return True

  @always_inline
  fn __ne__(self, vec: Self) -> Bool:
    return not self.__eq__(vec)

  @always_inline
  fn dim_match(self, vec: Self) -> Bool:
    '''Checks wether the length of the vector matches the length of `self`.'''
    if vec.data.dim(0) != self.data.dim(0): return False
    return True

  fn append(inout self, data: SIMD[type,1]) raises:
    self.data = self.data.reshape(
      TensorShape(
        self.data.shape().num_elements()+1
        )
      )
    self.data.simd_store(-1, data)

  fn sum(self) -> SIMD[type,1]:
    var val: SIMD[type,1] = 0
    for i in range(self.data.dim(0)):
      val += self.data[i]
    return val

  fn random_vector(inout self, len: Int) raises:
    '''Defines function for generating random vectors with `SIMD[type,1]` values.'''
    seed()
    self.data = rand[type](len)

  @staticmethod
  fn vector_applicable(func: fn(SIMD[type,1]) -> SIMD[type,1]) -> fn(Self) raises escaping -> Self:
    '''Takes a function and returns its equivalent for the `Self` type.'''
    @always_inline
    fn vecfunc(vec: Self) raises escaping -> Self:
      var val = Self()
      for i in vec:
        val.append(func(i))
      return val
    
    return vecfunc

  @staticmethod
  fn vector_applicable(func: fn(a:SIMD[type,1], x:SIMD[type,1]) -> SIMD[type,1]) -> fn(a:SIMD[type,1], vec:Self) raises escaping -> Self:
    '''Takes a function with a parameter and returns it equivalent for the `Self` type.'''
    @always_inline
    fn vecfunc(a:SIMD[type,1], vec: Self) raises -> Self:
      var val = Self()
      for i in vec:
        val.append(func(a, i))
      return val
    
    return vecfunc

alias MLVec = Vector[f32]
'''`Vector` containing `DType.float32` values, useful for ML applications.'''

alias GPUVec = Vector[tf32]
'''
`Vector` containing values of Mojo's special `DType.tensor_float32` type. 

Note that this requires a NVIDIA GPU.
'''

struct Matrix[type: DType](MDTensorWrapper):
  '''Defines a wrapper to handle mathematical matrices.'''

  var data: Tensor[type]
  var x: Int
  var y: Int

  fn __init__(inout self) raises:
    self.data = Tensor[type]()
    self.x = 0
    self.y = 0

  fn __init__(inout self, owned x: Int, owned y: Int):
    self.x = x
    self.y = y
    self.data = Tensor[type](x, y)

  fn __init__(inout self, owned data: Tensor[type]) raises:
    if data.rank() != 2:
      raise Error("Unable to convert given data to a matrix due to rank mismatch")
    self.data = data
    self.x = data.dim(0)
    self.y = data.dim(1)

  fn __copyinit__(inout self, borrowed other: Self):
    self.data = other.data
    self.x = other.x
    self.y = other.y

  fn __moveinit__(inout self, owned other: Self):
    self.data = other.data
    self.x = other.x
    self.y = other.y

  @always_inline
  fn __mul__(self, owned vec: Vector[type]) raises-> Vector[type]:
    '''Defines function for matrix-vector multiplication.'''
    if vec.data.dim(0) != self.data.dim(1):
      raise Error("Cannot multiply due to dimension mismatch")

    var val = Vector[type](self.data.dim(0))

    for i in range(self.data.dim(0)):
      # multiplies input vector with rows of matrix
      val.data[i] = vec * 
        Vector(
          # returns the i-th row of the matrix as Tensor value
          self.data.clip(
            self.data.dim(1)*i, 
            self.data.dim(1)*(i+1)-1
          )
        )
    
    return val

  @always_inline
  fn __rmul__(self, vec: Vector[type]) raises-> Vector[type]:
    '''Defines function for matrix-vector multiplication.'''
    return self.__mul__(vec)

  fn flatten(inout self) raises -> Vector[type]:
    '''Returns all values of the underlying `Tensor` value in a `Vector`.'''
    return Vector[type](
      self.data.reshape(
        TensorShape(
          self.x * self.y
          )
        )
      )

  fn random_matrix(inout self, x: Int, y: Int) raises:
    '''Defines function for generating random matrices with `Float64` values.'''
    seed()
    self.data = rand[type](x, y)

  @staticmethod
  fn matrix_applicable[type: DType](
    func: fn(SIMD[type, 1]) -> SIMD[type, 1]
    ) -> 
    fn(Matrix[type]) raises escaping -> Matrix[type]:
      
    '''Takes a function with a parameter and returns it equivalent for the `Matrix` type.'''
    @always_inline
    fn matfunc(matrix: Matrix[type]) raises escaping -> Matrix[type]:
      var val = Matrix[type](matrix.x, matrix.y)
      for i in range(matrix.x):
        for j in range(matrix.y):
          
          val.data.simd_store[1](
            VariadicList(i, j), 
            func(
              matrix.data.simd_load[1](i, j)
              )
            )

      return val
    
    return matfunc

alias MLMatrix = Matrix[f32]
'''`Matrix` containing `DType.float32` values, useful for ML applications.'''

alias GPUMatrix = Matrix[tf32]
'''
`Matrix` containing values of Mojo's special `DType.tensor_float32` type. 

Note that this requires a NVIDIA GPU.
'''

struct Tensor3D[type: DType](MDTensorWrapper):
  '''Defines a wrapper to handle mathematical 3-dimensional tensors.'''
  
  var x: Int
  var y: Int
  var z: Int
  var data: Tensor[type]

  fn __init__(inout self) raises:
    self.data = Tensor[type]()
    self.x = 0
    self.y = 0
    self.z = 0

  fn __init__(inout self, owned x: Int, owned y: Int, owned z: Int):
    self.x = x
    self.y = y
    self.z = z
    self.data = Tensor[type](x,y,z)

  fn __init__(inout self, owned data: Tensor[type]) raises:
    
    if data.rank() != 3:
      raise Error("Unable to convert given data to a matrix due to rank mismatch")
    self.data = data
    self.x = data.dim(0)
    self.y = data.dim(1)
    self.z = data.dim(2)

  fn __copyinit__(inout self, borrowed other: Self):
    self.data = other.data
    self.x = other.x
    self.y = other.y
    self.z = other.z

  fn __moveinit__(inout self, owned other: Self):
    self.data = other.data
    self.x = other.x
    self.y = other.y
    self.z = other.z

  fn flatten(inout self) raises -> Vector[type]:
    return Vector(self.data.reshape(TensorShape(self.x * self.y * self.z)))

  fn random_tensor3d(inout self, owned x: Int, owned y: Int, owned z: Int):
    seed()
    self.data = rand[type](x, y, z)

alias MLT3D = Tensor3D[f32]
'''`Tensor3D` containing `DType.float32` values, useful for ML applications.'''

alias GPUT3D = Tensor3D[tf32]
'''
`Tensor3D` containing values of Mojo's special `DType.tensor_float32` type. 

Note that this requires a NVIDIA GPU.
'''
